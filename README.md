# Sign Language Translator V1
In this project, machine learning methods will be used to create a sign language translator. This project transforms sign language movements into english text using the input from the device which was enabled with a camera. TensorFlow framework and the Python programming language are used to build the project. 

## Requirements
- [ ] Python 3.6 or higher
- [ ] TensorFlow
- [ ] Media pipeline
- [ ] OpenCV
- [ ] NumPy


## Dataset
The project uses a custom dataset collected from various sources and personally took from my hand gestures. The dataset contains a total of **1000+** images for each gesture in the sign language alphabet and numerical digits. The data has been pre-processed and augmented to improve the model's performance. The guidelines mentioned in this [image](https://drive.google.com/file/d/1GNe96f-HU6AiwAhRudMFzeC1Fnyrh5aW/view?usp=sharing) were used as reference sign language to train the model.



## Model
The project uses a Convolutional Neural Network (CNN) model to classify the sign language gestures. The model is trained using the collected dataset, and the weights are saved after training.


## Contribution
Contributions are welcome! If you want to contribute to the project, please create a pull request with a detailed explanation of the changes.

> We made this project as a team and collected data sets from various sources and analyzed and then generated data sets by our hands as input. Some sample data set were given too in this GitHub repository. We [P.V.Chaitanya](https://github.com/pvchaitanya8), [N.Suryakala](https://github.com/suryakala-1a), [P.G.Alekya](https://github.com/Gowri2003Alekya) worked as a team towards this project.

